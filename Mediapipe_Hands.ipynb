{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwu0918/yolov7-colab/blob/main/Mediapipe_Hands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PlazN5Q94l"
      },
      "source": [
        "Usage example of MediaPipe Hands Solution API in Python (see also http://solutions.mediapipe.dev/hands)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuQfLvpuJkb0"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWGG66XeRXEi"
      },
      "source": [
        "Upload any image that contains hand(s) to the Colab. We took two examples from the web: https://unsplash.com/photos/QyCH5jwrD_A and https://unsplash.com/photos/mt2fyrdXxzk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg"
      ],
      "metadata": {
        "id": "MRrIZorGR7Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SwzZ1nJLWt0"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2TjFyhLvVH"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "# Read images with OpenCV.\n",
        "images = {name: cv2.imread(name) for name in uploaded.keys()}\n",
        "# Preview the images.\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fso4oNqXIkp"
      },
      "source": [
        "All MediaPipe Solutions Python API examples are under mp.solutions.\n",
        "\n",
        "For the MediaPipe Hands solution, we can access this module as `mp_hands = mp.solutions.hands`.\n",
        "\n",
        "You may change the parameters, such as `static_image_mode`, `max_num_hands`, and `min_detection_confidence`, during the initialization. Run `help(mp_hands.Hands)` to get more informations about the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BboTB-FAMfPo"
      },
      "source": [
        "import mediapipe as mp\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "help(mp_hands.Hands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
      ],
      "metadata": {
        "id": "POqKPbgfRbiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 步驟一：導入必要函式庫\n",
        "# import mediapipe as mp\n",
        "# from mediapipe.tasks import python\n",
        "# from mediapipe.tasks.python import vision\n",
        "\n",
        "# # 步驟二：建立手部特徵點物件\n",
        "# # 載入手部特徵點偵測模型\n",
        "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "# # 建立手部特徵點偵測基本參數\n",
        "# options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2)\n",
        "# # 建立手部特徵點偵測器\n",
        "# detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "# # 步驟三：載入測試影像（可自行修改成待測試影像名稱）\n",
        "# image = mp.Image.create_from_file('kira-auf-der-heide-QyCH5jwrD_A-unsplash.jpg')\n",
        "\n",
        "# # 步驟四：偵測手部特徵點\n",
        "# detection_result = detector.detect(image)\n",
        "\n",
        "# # 步驟五：產生結果影像並顯示\n",
        "# # 將偵測到的結果（特徵點及連結線）繪製到新影像上\n",
        "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
        "# # 顯示時需將色彩格式RGB轉回BGR才能正確顯示。\n",
        "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "xmuO7QuvQ66k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = ['image.jpg']\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "  for idx, file in enumerate(IMAGE_FILES):\n",
        "    # Read an image, flip it around y-axis for correct handedness output (see\n",
        "    # above).\n",
        "    image = cv2.flip(cv2.imread(file), 1)\n",
        "    # Convert the BGR image to RGB before processing.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Print handedness and draw hand landmarks on the image.\n",
        "    print('Handedness:', results.multi_handedness)\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    image_height, image_width, _ = image.shape\n",
        "    annotated_image = image.copy()\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      print('hand_landmarks:', hand_landmarks)\n",
        "      print(\n",
        "          f'Index finger tip coordinates: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "    cv2.imwrite(\n",
        "        './annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
        "    resize_and_show(cv2.flip(annotated_image, 1))"
      ],
      "metadata": {
        "id": "IStlOPoMWktQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAivyQ_xOtFp"
      },
      "source": [
        "# Run MediaPipe Hands.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB, flip the image around y-axis for correct\n",
        "    # handedness output and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
        "\n",
        "    # Print handedness (left v.s. right hand).\n",
        "    print(f'Handedness of {name}:')\n",
        "    print(results.multi_handedness)\n",
        "    # print(results.multi_handedness[0].classification[0].label)\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    # Draw hand landmarks of each hand.\n",
        "    print(f'Hand landmarks of {name}:')\n",
        "    image_height, image_width, _ = image.shape\n",
        "    annotated_image = cv2.flip(image.copy(), 1)\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      # Print index finger tip coordinates.\n",
        "      print(\n",
        "          f'Index finger tip coordinate: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          hand_landmarks,\n",
        "          mp_hands.HAND_CONNECTIONS,\n",
        "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "          mp_drawing_styles.get_default_hand_connections_style())\n",
        "    resize_and_show(cv2.flip(annotated_image, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAchzK23Uabf"
      },
      "source": [
        "# Run MediaPipe Hands and plot 3d hands world landmarks.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # Draw hand world landmarks.\n",
        "    print(f'Hand world landmarks of {name}:')\n",
        "    if not results.multi_hand_world_landmarks:\n",
        "      continue\n",
        "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "      mp_drawing.plot_landmarks(\n",
        "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colabcam"
      ],
      "metadata": {
        "id": "xaj2i3nWdLZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colabcam\n",
        "colabcam.take_photo(\"1.jpg\")"
      ],
      "metadata": {
        "id": "W5c2GHMrdWxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝MQTT套件\n",
        "\n",
        "```\n",
        "!pip install paho-mqtt\n",
        "```\n",
        "或者\n",
        "```\n",
        "!pip3 install paho-mqtt\n",
        "```"
      ],
      "metadata": {
        "id": "qLR7DxPurmEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paho-mqtt\n",
        "# 或者\n",
        "# !pip3 install paho-mqtt"
      ],
      "metadata": {
        "id": "KwuaZHm_rhws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import paho.mqtt.client as mqtt #import the client1\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import ssl\n",
        "import certifi\n",
        "\n",
        "# 設置日期時間的格式\n",
        "ISOTIMEFORMAT = '%y-%m-%d %H:%M:%S'\n",
        "\n",
        "# MQTT\n",
        "mqtt_client_name = \"\"\n",
        "# mqtt_host = \"broker.emqx.io\"\n",
        "mqtt_host = \"mqttgo.io\"\n",
        "# mqtt_host = \"mqtt.xplatform.tranx.io\"\n",
        "# mqtt_ssl_host = \"mqttssl.xplatform.tranx.io\"\n",
        "\n",
        "# discovery_prefix = API_Key\n",
        "discovery_prefix = \"ccwu0918\"\n",
        "# component = 'sensor'\n",
        "node_id = 'video'\n",
        "# node_id = 'video'\n",
        "mqtt_topic_template = discovery_prefix + \"/\"  + node_id\n",
        "\n",
        "def on_log(client, userdata, level, buf):\n",
        "  print(\"log: \", buf)\n",
        "\n",
        "# 連線設定\n",
        "# 初始化地端程式\n",
        "client = mqtt.Client(mqtt_client_name)\n",
        "\n",
        "# 設定登入帳號密碼\n",
        "# client.username_pw_set(\"User_Name\", \"User_Password\")\n",
        "# client.username_pw_set(User_Name, API_Key)\n",
        "\n",
        "# client.on_log = on_log\n",
        "\n",
        "# 設定 SSL/TLS 加密連線\n",
        "# client.tls_set()\n",
        "# client.tls_set(\"/content/certificate.pem.crt\", tls_version=ssl.PROTOCOL_TLSv1_2)\n",
        "# client.tls_set(certifi.where())\n",
        "# client.tls_insecure_set(True)\n",
        "\n",
        "# 設定連線資訊(IP, Port, 連線時間)\n",
        "# client.connect(\"mqtt.xplatform.tranx.io\", 1883, 60)\n",
        "# client.connect(mqtt_ssl_host, port=8883, keepalive=60)\n",
        "client.connect(mqtt_host, port=1883, keepalive=60)\n",
        "\n",
        "# while True:\n",
        "#   mqtt_topic = str( mqtt_topic_template)\n",
        "#   payload_dict = {}\n",
        "#   Temperature = random.randint(20, 30)\n",
        "#   payload_dict['Temperature'] = Temperature\n",
        "#   Humidity = random.randint(50, 70)\n",
        "#   payload_dict['Humidity'] = Humidity\n",
        "#   payload_json = json.dumps(payload_dict)\n",
        "#   mqtt_payload = str(payload_json)\n",
        "\n",
        "#   print(\"mqtt_topic:\" + mqtt_topic)\n",
        "#   print(\"mqtt_payload:\" + mqtt_payload)\n",
        "\n",
        "#   client.publish(mqtt_topic, mqtt_payload)\n",
        "\n",
        "#   # 等待10秒發送一次資料\n",
        "#   # print(\"sleep 10 seconds.\")\n",
        "#   time.sleep(10)\n",
        "#   # print(\"printed after 10 seconds.\")"
      ],
      "metadata": {
        "id": "-gBKz-FRr5eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:bbox是另外疊加顯示的，速度會有延遲是正常的\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import mediapipe as mp\n",
        "import colabcam\n",
        "import numpy as np\n",
        "\n",
        "# For webcam input:\n",
        "# cap = cv2.VideoCapture(0)\n",
        "\n",
        "############## 各參數設定 ##################\n",
        "pTime  = 0\n",
        "minPwm = 0\n",
        "maxPwm = 255\n",
        "briArd = 0\n",
        "briBar = 400\n",
        "briPer = 0\n",
        "\n",
        "# start streaming video from webcam\n",
        "colabcam.video_stream()\n",
        "# label for video\n",
        "label_html = '顯示中...(點擊畫面以結束顯示)'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "zeros = np.zeros((480, 640, 1))\n",
        "\n",
        "title = '' # 存放手勢名稱及置信度字串\n",
        "gesture_name = '' #存放手勢名稱\n",
        "\n",
        "with mp_hands.Hands(\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as hands:\n",
        "\n",
        "    while True:\n",
        "        js_reply = colabcam.video_frame(label_html, bbox)\n",
        "        if not js_reply: break\n",
        "\n",
        "        # while cap.isOpened():\n",
        "        # success, image = cap.read()\n",
        "        # if not success:\n",
        "        #   print(\"Ignoring empty camera frame.\")\n",
        "        #   # If loading a video, use 'break' instead of 'continue'.\n",
        "        #   continue\n",
        "\n",
        "        # convert JS response to OpenCV Image\n",
        "        image = colabcam.js_to_image(js_reply[\"img\"])\n",
        "\n",
        "        # Flip the image horizontally for a later selfie-view display, and convert\n",
        "        # the BGR image to RGB.\n",
        "        # image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "        # To improve performance, optionally mark the image as not writeable to\n",
        "        # pass by reference.\n",
        "        results = hands.process(image)\n",
        "        # results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
        "        # gesture_name = results.multi_handedness[0].classification[0].label\n",
        "\n",
        "        # Draw the hand annotations on the image.\n",
        "        image.flags.writeable = True\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        frame_height, frame_width = np.shape(image)[0:2]\n",
        "\n",
        "        # tmpImg = np.zeros([frame_height, frame_width, 4], dtype=np.uint8)\n",
        "        overlapImg = np.zeros([frame_height, frame_width, 3], dtype=np.uint8)\n",
        "\n",
        "        # 計算每秒跑幾張\n",
        "        cTime = time.time()\n",
        "        fps = 1 / (cTime - pTime)\n",
        "        pTime = cTime\n",
        "        lmList = []\n",
        "\n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "\n",
        "                for id, lm in enumerate(hand_landmarks.landmark):\n",
        "                    # print(id, lm)\n",
        "                    h, w, c = image.shape\n",
        "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
        "                    # print(id, cx, cy)\n",
        "                    lmList.append( [id, cx, cy])\n",
        "\n",
        "                    # if draw:\n",
        "                    #     cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
        "\n",
        "                    # print('hand_landmarks:', hand_landmarks)\n",
        "                    # print(\n",
        "                    #     f'Index finger tip coordinates: (',\n",
        "                    #     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "                    #     f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
        "                    # )\n",
        "\n",
        "                # mp_drawing.draw_landmarks(\n",
        "                #    overlapImg, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    overlapImg,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                    mp_drawing_styles.get_default_hand_connections_style())\n",
        "            # cv2.imshow('MediaPipe Hands', image)\n",
        "            # overlapImg = cv2.rectangle(overlapImg,(40,40),(60,60),(255,0,0),2)\n",
        "            # cv2.putText(overlapImg,'text test',(20,20),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
        "            # resize_and_show(cv2.flip(overlapImg, 1))\n",
        "\n",
        "            if len(lmList) != 0:\n",
        "                x1, y1 = lmList[4][1], lmList[4][2]\n",
        "                x2, y2 = lmList[8][1], lmList[8][2]\n",
        "                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "                cv2.circle(overlapImg, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
        "                cv2.circle(overlapImg, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
        "\n",
        "                #計算大拇指和食指的直線中點距離\n",
        "                cv2.line(overlapImg, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
        "                cv2.circle(overlapImg, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
        "\n",
        "                #計算大拇指和食指的直線距離\n",
        "                length = math.hypot(x2 - x1, y2 - y1)\n",
        "                #print(length)\n",
        "\n",
        "                #將大拇指和食指的直線距離換算成0~255，Arduino PWM控制數值亦為0~255\n",
        "                brightness = np.interp(length, [50, 300], [minPwm, maxPwm])\n",
        "\n",
        "                briBar = np.interp(length, [50, 300], [400, 150])\n",
        "                briArd = np.around(brightness,2)\n",
        "\n",
        "\n",
        "                if length < 50:\n",
        "                    cv2.circle(overlapImg, (cx, cy), 15, (0, 255, 0), cv2.FILLED)\n",
        "\n",
        "            #畫出直方圖\n",
        "            cv2.rectangle(overlapImg,(50,150),(85,400),(255, 0, 255),3)\n",
        "            cv2.rectangle(overlapImg,(50,150),(85,400),(255, 0, 255),3)\n",
        "            cv2.rectangle(overlapImg,(50, int(briBar)),(85,400),(255, 0, 255),cv2.FILLED)\n",
        "            cv2.putText(overlapImg, f'brightness: {int(briArd)}', (15, 140), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 255), 3)\n",
        "\n",
        "        # 將BGR影像轉換為BGRA格式\n",
        "        bgra = cv2.cvtColor(overlapImg, cv2.COLOR_BGR2BGRA)\n",
        "        cv2.putText(bgra, f'FPS: {int(fps)}', (40, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 3)\n",
        "        bgra[:,:,3] = (bgra.max(axis = 2) > 0 ).astype(int) * 0\n",
        "            # tmp = np.concatenate((overlapImg, np.zeros((480, 640, 1))))\n",
        "\n",
        "        ret, data = cv2.imencode('.jpg', bgra)   # compress array of pixels to JPG data\n",
        "        # data = b64encode(data)                  # encode base64\n",
        "        # data = data.decode()\n",
        "\n",
        "        mqtt_payload = data.tobytes()\n",
        "        # mqtt_payload = bytearray(data)\n",
        "        mqtt_topic = \"ccwu0918/video\"\n",
        "\n",
        "        # print(\"mqtt_topic:\" + mqtt_topic)\n",
        "        # print(\"mqtt_payload:\" + str(mqtt_payload))\n",
        "\n",
        "        client.publish(mqtt_topic, mqtt_payload)\n",
        "\n",
        "        mqtt_topic = \"ccwu0918/brightness\"\n",
        "        client.publish(mqtt_topic, briArd)\n",
        "\n",
        "        # mqtt_topic = \"ccwu0918/gesture\"\n",
        "        # client.publish(mqtt_topic, gesture_name)\n",
        "\n",
        "        # 等待10秒發送一次資料\n",
        "        # print(\"sleep 10 seconds.\")\n",
        "        time.sleep(1)\n",
        "        # print(\"printed after 10 seconds.\")\n",
        "\n",
        "        bbox = colabcam.cvImg2bbox(bgra)\n",
        "\n",
        "    # Flip the image horizontally for a later selfie-view display, and convert\n",
        "    # the BGR image to RGB.\n",
        "    # image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    # image.flags.writeable = False\n",
        "    # results = hands.process(image)\n",
        "\n",
        "    # # Draw the hand annotations on the image.\n",
        "    # image.flags.writeable = True\n",
        "    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    # if results.multi_hand_landmarks:\n",
        "    #   for hand_landmarks in results.multi_hand_landmarks:\n",
        "    #     mp_drawing.draw_landmarks(\n",
        "    #         image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    # cv2.imshow('MediaPipe Hands', image)\n",
        "    # if cv2.waitKey(5) & 0xFF == 27:\n",
        "    #   break"
      ],
      "metadata": {
        "id": "xdH1unzRYqR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}